{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 5386), started 0:16:07 ago. (Use '!kill 5386' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-7be2396cd12c4e61\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-7be2396cd12c4e61\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATASET_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_FILE_PATH = f'{DATASET_DIR}/aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.isfile(DATASET_FILE_PATH):\n",
    "    print(f'Downloading dataset into {DATASET_FILE_PATH} ...')\n",
    "    with urllib.request.urlopen(DATASET_URL) as response, open(DATASET_FILE_PATH, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "else:\n",
    "    print('Dataset already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Untar the dataset archive\n",
    "if not os.path.isdir(f'{DATASET_DIR}/aclImdb'):\n",
    "    with tarfile.open(DATASET_FILE_PATH) as archive:\n",
    "        print(f'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...')\n",
    "        archive.extractall(DATASET_DIR)\n",
    "        print('Extraction finished.')\n",
    "else:\n",
    "    print('Dataset already extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "VOCAB_SIZE = 1_000\n",
    "MAX_SENTENCE_LEN = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "TEST_FOLDER = f'{DATASET_DIR}/aclImdb/test'\n",
    "TEST_POSITIVE_FOLDER = f'{TEST_FOLDER}/pos'\n",
    "TEST_NEGATIVE_FOLDER = f'{TEST_FOLDER}/neg'\n",
    "\n",
    "TRAIN_FOLDER = f'{DATASET_DIR}/aclImdb/train'\n",
    "TRAIN_POSITIVE_FOLDER = f'{TRAIN_FOLDER}/pos'\n",
    "TRAIN_NEGATIVE_FOLDER = f'{TRAIN_FOLDER}/neg'\n",
    "TRAIN_UNSUPERVISED_FOLDER = f'{TRAIN_FOLDER}/unsup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab_file, vocab_size, separator='\\n'):\n",
    "    vocab = open(vocab_file).read().split(separator)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size, oov_token=vocab_size)\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(f'{DATASET_DIR}/aclImdb/imdb.vocab', VOCAB_SIZE+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shifted_dataset_from_files(folders, shuffle=True):\n",
    "    files = map(lambda folder: glob.glob(f'{folder}/*'), folders)\n",
    "\n",
    "    labeled_files = map(lambda files_per_folder:\n",
    "                        map(lambda file_path:\n",
    "                            [open(file_path).read().split(' ')[:-1], open(file_path).read().split(' ')[1:]]\n",
    "                        , files_per_folder)\n",
    "                    , files)\n",
    "\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          *tokenizer.texts_to_sequences([example[1]])],\n",
    "                         flat_labeled_files)\n",
    "    return list(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_dataset_from_files(folders, label_map={'pos':[1, 0], 'neg': [0, 1]}, shuffle=True):\n",
    "    files = map(lambda folder: [glob.glob(f'{folder}/*'), f'{folder}'], folders)\n",
    "\n",
    "    # Assign label to every files based on folder they are in\n",
    "    labeled_files = map(lambda files_with_label:\n",
    "                        map(lambda file_path:\n",
    "                            [file_path, files_with_label[1].split('/')[-1]] # Take only the last folde from the folder path\n",
    "                        , files_with_label[0])\n",
    "                    , files)\n",
    "\n",
    "    # flatten list\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    # read file contents\n",
    "    labeled_texts = map(lambda example: [open(example[0]).read().split(' ')[:MAX_SENTENCE_LEN], example[1]], flat_labeled_files)\n",
    "\n",
    "    # tokenize texts\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          label_map[example[1]]], labeled_texts)\n",
    "    return list(labeled_tokens), len(flat_labeled_files)\n",
    "\n",
    "cls_test_ds, num_test_samples = create_labeled_dataset_from_files([f'{TEST_POSITIVE_FOLDER}', f'{TEST_NEGATIVE_FOLDER}, {TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "cls_train_ds, num_train_samples = create_labeled_dataset_from_files([f'{TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "\n",
    "def cls_test_gen():\n",
    "    for el in cls_test_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "def cls_train_gen():\n",
    "    for el in cls_train_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(lambda: cls_test_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = tf.data.Dataset.from_generator(lambda: cls_train_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = ds_train.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)\n",
    "\n",
    "ds_test = ds_test.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps, validate for 781 steps\n",
      "Epoch 1/20\n",
      "781/781 [==============================] - 67s 85ms/step - loss: 0.6805 - accuracy: 0.5616 - val_loss: 0.6477 - val_accuracy: 0.6413\n",
      "Epoch 2/20\n",
      "781/781 [==============================] - 62s 80ms/step - loss: 0.6020 - accuracy: 0.6750 - val_loss: 0.5725 - val_accuracy: 0.7026\n",
      "Epoch 3/20\n",
      "781/781 [==============================] - 68s 88ms/step - loss: 0.5544 - accuracy: 0.7153 - val_loss: 0.5603 - val_accuracy: 0.7101\n",
      "Epoch 4/20\n",
      "781/781 [==============================] - 63s 81ms/step - loss: 0.5393 - accuracy: 0.7261 - val_loss: 0.5571 - val_accuracy: 0.7125\n",
      "Epoch 5/20\n",
      "781/781 [==============================] - 63s 81ms/step - loss: 0.5299 - accuracy: 0.7348 - val_loss: 0.5539 - val_accuracy: 0.7149\n",
      "Epoch 6/20\n",
      "781/781 [==============================] - 74s 94ms/step - loss: 0.5227 - accuracy: 0.7390 - val_loss: 0.5529 - val_accuracy: 0.7179\n",
      "Epoch 7/20\n",
      "781/781 [==============================] - 55s 70ms/step - loss: 0.5172 - accuracy: 0.7418 - val_loss: 0.5519 - val_accuracy: 0.7187\n",
      "Epoch 8/20\n",
      "781/781 [==============================] - 55s 70ms/step - loss: 0.5128 - accuracy: 0.7449 - val_loss: 0.5518 - val_accuracy: 0.7194\n",
      "Epoch 9/20\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.5092 - accuracy: 0.7462 - val_loss: 0.5502 - val_accuracy: 0.7199\n",
      "Epoch 10/20\n",
      "781/781 [==============================] - 65s 83ms/step - loss: 0.5056 - accuracy: 0.7483 - val_loss: 0.5493 - val_accuracy: 0.7207\n",
      "Epoch 11/20\n",
      "781/781 [==============================] - 56s 72ms/step - loss: 0.5023 - accuracy: 0.7499 - val_loss: 0.5483 - val_accuracy: 0.7218\n",
      "Epoch 12/20\n",
      "781/781 [==============================] - 59s 76ms/step - loss: 0.4990 - accuracy: 0.7526 - val_loss: 0.5477 - val_accuracy: 0.7230\n",
      "Epoch 13/20\n",
      "781/781 [==============================] - 64s 82ms/step - loss: 0.4956 - accuracy: 0.7546 - val_loss: 0.5460 - val_accuracy: 0.7237\n",
      "Epoch 14/20\n",
      "395/781 [==============>...............] - ETA: 27s - loss: 0.4961 - accuracy: 0.7578"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(8, activation='sigmoid')),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds_train,\n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        validation_data=ds_test,\n",
    "        steps_per_epoch=num_train_samples // BATCH_SIZE,\n",
    "        validation_steps=num_test_samples // BATCH_SIZE,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=os.path.join(\"logs\", \"baseline_small2\"),\n",
    "                histogram_freq=1,\n",
    "                profile_batch=0)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# while True:\n",
    "# \tprint('Enter something:')\n",
    "# \tinp = input()\n",
    "# \tprint(model.predict(tokenizer.texts_to_sequences([inp.split(' ')])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
