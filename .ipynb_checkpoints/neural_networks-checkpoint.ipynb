{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download dataset\n",
    "DATASET_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_FILE_PATH = f'{DATASET_DIR}/aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.isfile(DATASET_FILE_PATH):\n",
    "    print(f'Downloading dataset into {DATASET_FILE_PATH} ...')\n",
    "    with urllib.request.urlopen(DATASET_URL) as response, open(DATASET_FILE_PATH, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "else:\n",
    "    print('Dataset already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Untar the dataset archive\n",
    "if not os.path.isdir(f'{DATASET_DIR}/aclImdb'):\n",
    "    with tarfile.open(DATASET_FILE_PATH) as archive:\n",
    "        print(f'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...')\n",
    "        archive.extractall(DATASET_DIR)\n",
    "        print('Extraction finished.')\n",
    "else:\n",
    "    print('Dataset already extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "TEST_FOLDER = f'{DATASET_DIR}/aclImdb/test'\n",
    "TEST_POSITIVE_FOLDER = f'{TEST_FOLDER}/pos'\n",
    "TEST_NEGATIVE_FOLDER = f'{TEST_FOLDER}/neg'\n",
    "\n",
    "TRAIN_FOLDER = f'{DATASET_DIR}/aclImdb/train'\n",
    "TRAIN_POSITIVE_FOLDER = f'{TRAIN_FOLDER}/pos'\n",
    "TRAIN_NEGATIVE_FOLDER = f'{TRAIN_FOLDER}/neg'\n",
    "TRAIN_UNSUPERVISED_FOLDER = f'{TRAIN_FOLDER}/unsup'\n",
    "\n",
    "VOCAB_SIZE = 10_000\n",
    "MAX_SENTENCE_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "def get_tokenizer(vocab_file, vocab_size, separator='\\n'):\n",
    "    # FIXME: filter out duplicates, don't use set -> nondeterministic sorting, this example is OK imdb.vocab has unique values\n",
    "    vocab = open(vocab_file).read().split(separator) \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size, oov_token=0)\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    return tokenizer\n",
    "tokenizer = get_tokenizer(f'{DATASET_DIR}/aclImdb/imdb.vocab', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of positive reviews\n",
    "def create_shifted_dataset_from_files(folders, shuffle=True):\n",
    "    files = map(lambda folder: glob.glob(f'{folder}/*'), folders)\n",
    "\n",
    "    labeled_files = map(lambda files_per_folder:\n",
    "                        map(lambda file_path:\n",
    "                            [open(file_path).read().split(' ')[:-1], open(file_path).read().split(' ')[1:]]\n",
    "                        , files_per_folder)\n",
    "                    , files)\n",
    "\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          *tokenizer.texts_to_sequences([example[1]])],\n",
    "                         flat_labeled_files);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_dataset_from_files(folders, label_map={'pos':[1, 0], 'neg': [0, 1]}, shuffle=True):\n",
    "    files = map(lambda folder: [glob.glob(f'{folder}/*'), f'{folder}'], folders)\n",
    "\n",
    "    # Assign label to every files based on folder they are in\n",
    "    labeled_files = map(lambda files_with_label:\n",
    "                        map(lambda file_path:\n",
    "                            [file_path, files_with_label[1].split('/')[-1]] # Take only the last folde from the folder path\n",
    "                        , files_with_label[0])\n",
    "                    , files)\n",
    "\n",
    "    # flatten list\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    # read file contents\n",
    "    labeled_texts = map(lambda example: [open(example[0]).read().split(' ')[:MAX_SENTENCE_LEN], example[1]], flat_labeled_files)\n",
    "\n",
    "    # tokenize texts\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          label_map[example[1]]], labeled_texts)\n",
    "    return labeled_tokens\n",
    "\n",
    "cls_test_ds = create_labeled_dataset_from_files([f'{TEST_POSITIVE_FOLDER}', f'{TEST_NEGATIVE_FOLDER}'])\n",
    "cls_train_ds = create_labeled_dataset_from_files([f'{TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "\n",
    "def cls_test_gen():\n",
    "    for el in cls_test_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "def cls_train_gen():\n",
    "    for el in cls_train_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(cls_test_gen, (tf.int64, tf.int64))\n",
    "ds_train = tf.data.Dataset.from_generator(cls_train_gen, (tf.int64, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "ds_files_pos = glob.glob(f'{TRAIN_POSITIVE_FOLDER}/*.txt')\n",
    "ds_texts_pos = map(lambda fn: open(fn).read().split(' ')[:MAX_SENTENCE_LEN], ds_files_pos)\n",
    "ds_sequences_pos = tokenizer.texts_to_sequences(ds_texts_pos)\n",
    "\n",
    "def generator():\n",
    "    for el in ds_sequences_pos:\n",
    "        yield (el, [1, 0])\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(generator, (tf.int64, tf.int64))\n",
    "\n",
    "# Dataset of negative reviews\n",
    "ds_files_neg = glob.glob(f'{TRAIN_NEGATIVE_FOLDER}/*.txt')\n",
    "ds_texts_neg = map(lambda fn: open(fn).read().split(' ')[:MAX_SENTENCE_LEN], ds_files_neg)\n",
    "ds_sequences_neg = tokenizer.texts_to_sequences(ds_texts_neg)\n",
    "\n",
    "def gen_negative():\n",
    "    for el in ds_sequences_neg:\n",
    "        yield (el, [0, 1])\n",
    "\n",
    "ds_neg = tf.data.Dataset.from_generator(gen_negative, (tf.int64, tf.int64))\n",
    "\n",
    "# Creating the whole dataset\n",
    "def gen_all():\n",
    "    g1 = gen_negative()\n",
    "    g2 = generator()\n",
    "    while True:\n",
    "        val1 = next(g1, None)\n",
    "        val2 = next(g2, None)\n",
    "\n",
    "        if val1:\n",
    "            yield val1\n",
    "\n",
    "        if val2:\n",
    "            yield val2\n",
    "\n",
    "        if val2 == None and val1 == None:\n",
    "            break\n",
    "\n",
    "ds_whole = tf.data.Dataset.from_generator(gen_all, (tf.int64, tf.int64))\n",
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratívna analýza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozhodli sme sa použiť dataset recenzíí filmov z websídla IMDb dostupný na: http://ai.stanford.edu/~amaas/data/sentiment/ . Tento dataset obsahuje 25 000 označkovaných vysoko polarizovaných recenzíí a 25 000 neoznačnených recenzíí. Keďže dát je pomerne veľa, nemal by vzniknúť problém pri trénovaní modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_train_gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prekryv prvkov z train a test.\n",
    "Ci sa Neopakuju prvky.\n",
    "kolko trenovacich testovacich dat mame.\n",
    "explorativan analyza.\n",
    "min, mod, median -> dlzky reviews v pos a neg.\n",
    "\n",
    "add no. of start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ds = ds_whole\n",
    "\"\"\"\n",
    "ds = ds_test\n",
    "ds = ds.shuffle(buffer_size=10_000)\n",
    "# Bucketing, how the fuck do I sort padded batches ?\n",
    "ds = ds.apply(tf.data.experimental.bucket_by_sequence_length(\n",
    "    lambda el, _: tf.size(el),\n",
    "    [50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
    "    [32] * 15,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True\n",
    "))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(48, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds, epochs=1)\n",
    "\n",
    "while True:\n",
    "\tprint('Enter something:')\n",
    "\tinp = input()\n",
    "\tprint(model.predict(tokenizer.texts_to_sequences([inp.split(' ')])))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
