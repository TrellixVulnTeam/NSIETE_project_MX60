{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-28cfdf11bf4de1b\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-28cfdf11bf4de1b\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          url.port = 6006;\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "  "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir logs --bind_all\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATASET_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_FILE_PATH = f'{DATASET_DIR}/aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.isfile(DATASET_FILE_PATH):\n",
    "    print(f'Downloading dataset into {DATASET_FILE_PATH} ...')\n",
    "    with urllib.request.urlopen(DATASET_URL) as response, open(DATASET_FILE_PATH, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "else:\n",
    "    print('Dataset already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Untar the dataset archive\n",
    "if not os.path.isdir(f'{DATASET_DIR}/aclImdb'):\n",
    "    with tarfile.open(DATASET_FILE_PATH) as archive:\n",
    "        print(f'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...')\n",
    "        archive.extractall(DATASET_DIR)\n",
    "        print('Extraction finished.')\n",
    "else:\n",
    "    print('Dataset already extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "VOCAB_SIZE = 1_000\n",
    "MAX_SENTENCE_LEN = 50\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "TEST_FOLDER = f'{DATASET_DIR}/aclImdb/test'\n",
    "TEST_POSITIVE_FOLDER = f'{TEST_FOLDER}/pos'\n",
    "TEST_NEGATIVE_FOLDER = f'{TEST_FOLDER}/neg'\n",
    "\n",
    "TRAIN_FOLDER = f'{DATASET_DIR}/aclImdb/train'\n",
    "TRAIN_POSITIVE_FOLDER = f'{TRAIN_FOLDER}/pos'\n",
    "TRAIN_NEGATIVE_FOLDER = f'{TRAIN_FOLDER}/neg'\n",
    "TRAIN_UNSUPERVISED_FOLDER = f'{TRAIN_FOLDER}/unsup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab_file, vocab_size, separator='\\n'):\n",
    "    vocab = open(vocab_file).read().split(separator)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size, oov_token=vocab_size)\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(f'{DATASET_DIR}/aclImdb/imdb.vocab', VOCAB_SIZE+1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "We have 2 different way of data processing one for classification and one for language modeling. For classification we create pairs of reviews and their coresponding labels while for language modeling the input and output will be the same with the inputs being shifted one word to the left and the output one word to the right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shifted_dataset_from_files(folders, shuffle=True):\n",
    "    files = map(lambda folder: glob.glob(f'{folder}/*'), folders)\n",
    "\n",
    "    labeled_files = map(lambda files_per_folder:\n",
    "                        map(lambda file_path:\n",
    "                            [open(file_path).read().split(' ')[:-1], open(file_path).read().split(' ')[1:]]\n",
    "                        , files_per_folder)\n",
    "                    , files)\n",
    "\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          *tokenizer.texts_to_sequences([example[1]])],\n",
    "                         flat_labeled_files)\n",
    "    return list(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_dataset_from_files(folders, label_map={'pos':[1, 0], 'neg': [0, 1]}, shuffle=True):\n",
    "    files = map(lambda folder: [glob.glob(f'{folder}/*'), f'{folder}'], folders)\n",
    "\n",
    "    # Assign label to every files based on folder they are in\n",
    "    labeled_files = map(lambda files_with_label:\n",
    "                        map(lambda file_path:\n",
    "                            [file_path, files_with_label[1].split('/')[-1]] # Take only the last folde from the folder path\n",
    "                        , files_with_label[0])\n",
    "                    , files)\n",
    "\n",
    "    # flatten list\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    # read file contents\n",
    "    labeled_texts = map(lambda example: [open(example[0]).read().split(' ')[:MAX_SENTENCE_LEN], example[1]], flat_labeled_files)\n",
    "\n",
    "    # tokenize texts\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          label_map[example[1]]], labeled_texts)\n",
    "    return list(labeled_tokens), len(flat_labeled_files)\n",
    "\n",
    "cls_test_ds, num_test_samples = create_labeled_dataset_from_files([f'{TEST_POSITIVE_FOLDER}', f'{TEST_NEGATIVE_FOLDER}, {TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "cls_train_ds, num_train_samples = create_labeled_dataset_from_files([f'{TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "\n",
    "def cls_test_gen():\n",
    "    for el in cls_test_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "def cls_train_gen():\n",
    "    for el in cls_train_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(lambda: cls_test_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = tf.data.Dataset.from_generator(lambda: cls_train_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = ds_train.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)\n",
    "\n",
    "ds_test = ds_test.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline models\n",
    "2 models were trained. The first one implements a simple BiLSTM with a dense layer at the end. The second one is a single LSTM with self-attention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BiLSTM\n",
    "\n",
    "|parameter|value|\n",
    "|-----|-----|\n",
    "|embedding size|256|\n",
    "|dictionary length|10K|\n",
    "|sequence length in words|500|\n",
    "|BiLSTM|256|\n",
    "|batch size|128|\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(8, activation='sigmoid')),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds_train,\n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        validation_data=ds_test,\n",
    "        steps_per_epoch=num_train_samples // BATCH_SIZE,\n",
    "        validation_steps=num_test_samples // BATCH_SIZE,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=os.path.join(\"logs\", \"bilstm\"),\n",
    "                histogram_freq=1,\n",
    "                profile_batch=0)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BiLSTM](assets/bilstm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram of the architecture\n",
    "![BiLSTM](assets/bilstm-diag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Self-attention\n",
    "    \n",
    "|parameter|value|\n",
    "|-----|-----|\n",
    "|embedding size|256|\n",
    "|dictionary length|10K|\n",
    "|sequence length in words|500|\n",
    "|LSTM|256|\n",
    "|Attention size|128|\n",
    "|batch size|128|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import layers\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(8, activation='sigmoid', return_sequences=True),\n",
    "    layers.SelfAttention(size=64,\n",
    "                    num_hops=32,\n",
    "                    use_penalization=False,\n",
    "                    model_api='sequential')\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds_train,\n",
    "        epochs=20,\n",
    "        shuffle=True,\n",
    "        validation_data=ds_test,\n",
    "        steps_per_epoch=num_train_samples // BATCH_SIZE,\n",
    "        validation_steps=num_test_samples // BATCH_SIZE,\n",
    "        callbacks=[\n",
    "            tf.keras.callbacks.TensorBoard(\n",
    "                log_dir=os.path.join(\"logs\", \"attn\"),\n",
    "                histogram_freq=1,\n",
    "                profile_batch=0)\n",
    "        ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SelfAttn](assets/attn.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram of the architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![BiLSTM](assets/attn-diag.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "## Most likely\n",
    "- Hierarchical LSTM\n",
    "- attention-before and after\n",
    "- embeddings\n",
    "- comparison of different modeles\n",
    "\n",
    "## A little bit less likely\n",
    "- generating stuff\n",
    "- language models\n",
    "- rating prediction\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
