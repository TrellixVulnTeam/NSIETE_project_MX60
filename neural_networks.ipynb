{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-cc937eab00e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mDATASET_FILE_PATH\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf'{DATASET_DIR}/aclImdb_v1.tar.gz'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Downloading dataset into {DATASET_FILE_PATH} ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_URL\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_FILE_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mout_file\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATASET_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_FILE_PATH = f'{DATASET_DIR}/aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.isfile(DATASET_FILE_PATH):\n",
    "    print(f'Downloading dataset into {DATASET_FILE_PATH} ...')\n",
    "    with urllib.request.urlopen(DATASET_URL) as response, open(DATASET_FILE_PATH, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "else:\n",
    "    print('Dataset already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-d0886d351060>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Untar the dataset archive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'{DATASET_DIR}/aclImdb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtarfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_FILE_PATH\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0marchive\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0marchive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextractall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDATASET_DIR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "# Untar the dataset archive\n",
    "if not os.path.isdir(f'{DATASET_DIR}/aclImdb'):\n",
    "    with tarfile.open(DATASET_FILE_PATH) as archive:\n",
    "        print(f'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...')\n",
    "        archive.extractall(DATASET_DIR)\n",
    "        print('Extraction finished.')\n",
    "else:\n",
    "    print('Dataset already extracted.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "TEST_FOLDER = f'{DATASET_DIR}/aclImdb/test'\n",
    "TEST_POSITIVE_FOLDER = f'{TEST_FOLDER}/pos'\n",
    "TEST_NEGATIVE_FOLDER = f'{TEST_FOLDER}/neg'\n",
    "\n",
    "TRAIN_FOLDER = f'{DATASET_DIR}/aclImdb/train'\n",
    "TRAIN_POSITIVE_FOLDER = f'{TRAIN_FOLDER}/pos'\n",
    "TRAIN_NEGATIVE_FOLDER = f'{TRAIN_FOLDER}/neg'\n",
    "TRAIN_UNSUPERVISED_FOLDER = f'{TRAIN_FOLDER}/unsup'\n",
    "\n",
    "VOCAB_SIZE = 10_000\n",
    "MAX_SENTENCE_LEN = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tokenizer\n",
    "def get_tokenizer(vocab_file, vocab_size, separator='\\n'):\n",
    "    # FIXME: filter out duplicates, don't use set -> nondeterministic sorting, this example is OK imdb.vocab has unique values\n",
    "    vocab = open(vocab_file).read().split(separator) \n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size, oov_token=0)\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    return tokenizer\n",
    "tokenizer = get_tokenizer(f'{DATASET_DIR}/aclImdb/imdb.vocab', VOCAB_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset of positive reviews\n",
    "def create_shifted_dataset_from_files(folders, shuffle=True):\n",
    "    files = map(lambda folder: glob.glob(f'{folder}/*'), folders)\n",
    "\n",
    "    labeled_files = map(lambda files_per_folder:\n",
    "                        map(lambda file_path:\n",
    "                            [open(file_path).read().split(' ')[:-1], open(file_path).read().split(' ')[1:]]\n",
    "                        , files_per_folder)\n",
    "                    , files)\n",
    "\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          *tokenizer.texts_to_sequences([example[1]])],\n",
    "                         flat_labeled_files);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_dataset_from_files(folders, label_map={'pos':[1, 0], 'neg': [0, 1]}, shuffle=True):\n",
    "    files = map(lambda folder: [glob.glob(f'{folder}/*'), f'{folder}'], folders)\n",
    "\n",
    "    # Assign label to every files based on folder they are in\n",
    "    labeled_files = map(lambda files_with_label:\n",
    "                        map(lambda file_path:\n",
    "                            [file_path, files_with_label[1].split('/')[-1]] # Take only the last folde from the folder path\n",
    "                        , files_with_label[0])\n",
    "                    , files)\n",
    "\n",
    "    # flatten list\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    # read file contents\n",
    "    labeled_texts = map(lambda example: [open(example[0]).read().split(' ')[:MAX_SENTENCE_LEN], example[1]], flat_labeled_files)\n",
    "\n",
    "    # tokenize texts\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          label_map[example[1]]], labeled_texts)\n",
    "    return labeled_tokens\n",
    "\n",
    "cls_test_ds = create_labeled_dataset_from_files([f'{TEST_POSITIVE_FOLDER}', f'{TEST_NEGATIVE_FOLDER}'])\n",
    "cls_train_ds = create_labeled_dataset_from_files([f'{TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "\n",
    "def cls_test_gen():\n",
    "    for el in cls_test_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "def cls_train_gen():\n",
    "    for el in cls_train_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(cls_test_gen, (tf.int64, tf.int64))\n",
    "ds_train = tf.data.Dataset.from_generator(cls_train_gen, (tf.int64, tf.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########################################################################################################################\n",
    "\n",
    "ds_files_pos = glob.glob(f'{TRAIN_POSITIVE_FOLDER}/*.txt')\n",
    "ds_texts_pos = map(lambda fn: open(fn).read().split(' ')[:MAX_SENTENCE_LEN], ds_files_pos)\n",
    "ds_sequences_pos = tokenizer.texts_to_sequences(ds_texts_pos)\n",
    "\n",
    "def generator():\n",
    "    for el in ds_sequences_pos:\n",
    "        yield (el, [1, 0])\n",
    "\n",
    "ds = tf.data.Dataset.from_generator(generator, (tf.int64, tf.int64))\n",
    "\n",
    "# Dataset of negative reviews\n",
    "ds_files_neg = glob.glob(f'{TRAIN_NEGATIVE_FOLDER}/*.txt')\n",
    "ds_texts_neg = map(lambda fn: open(fn).read().split(' ')[:MAX_SENTENCE_LEN], ds_files_neg)\n",
    "ds_sequences_neg = tokenizer.texts_to_sequences(ds_texts_neg)\n",
    "\n",
    "def gen_negative():\n",
    "    for el in ds_sequences_neg:\n",
    "        yield (el, [0, 1])\n",
    "\n",
    "ds_neg = tf.data.Dataset.from_generator(gen_negative, (tf.int64, tf.int64))\n",
    "\n",
    "# Creating the whole dataset\n",
    "def gen_all():\n",
    "    g1 = gen_negative()\n",
    "    g2 = generator()\n",
    "    while True:\n",
    "        val1 = next(g1, None)\n",
    "        val2 = next(g2, None)\n",
    "\n",
    "        if val1:\n",
    "            yield val1\n",
    "\n",
    "        if val2:\n",
    "            yield val2\n",
    "\n",
    "        if val2 == None and val1 == None:\n",
    "            break\n",
    "\n",
    "ds_whole = tf.data.Dataset.from_generator(gen_all, (tf.int64, tf.int64))\n",
    "##########################################################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratívna analýza"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rozhodli sme sa použiť dataset recenzíí filmov z websídla IMDb dostupný na: http://ai.stanford.edu/~amaas/data/sentiment/ . Tento dataset obsahuje 25 000 označkovaných vysoko polarizovaných recenzíí a 25 000 neoznačnených recenzíí. Keďže dát je pomerne veľa, nemal by vzniknúť problém pri trénovaní modelu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: (<unknown>, <unknown>), types: (tf.int64, tf.int64)>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_whole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(TensorSpec(shape=<unknown>, dtype=tf.int64, name=None),\n",
       " TensorSpec(shape=<unknown>, dtype=tf.int64, name=None))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds_whole.element_spec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------\n",
      "tf.Tensor(\n",
      "[1010  125   26   14    4  276 9253    1  148   29 9081    6   44    1\n",
      "    6    1    1 9269  314 2722    6   10    1    1    1    1 9416    4\n",
      "   29  335   63    1    1   11    1   10  652  219   18   50    1], shape=(41,), dtype=int64)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n",
      "------\n",
      "tf.Tensor(\n",
      "[  40   10   17  160 1803   52 4109  336  982 1411   10 2516   14 4068\n",
      "    1 9822   40  148    1 2564   10   17  714 1857    1  169 8968    1\n",
      "    1    1  169   10  106    1    4    1  126  169  385 9187    1  275\n",
      "   40 9188    1    1    1    1   64   10  731    1], shape=(52,), dtype=int64)\n",
      "tf.Tensor([1 0], shape=(2,), dtype=int64)\n",
      "------\n",
      "tf.Tensor(\n",
      "[   1    1    4  829 2546   31    8    1   11   69 1424    4   17  457\n",
      " 2555    6    1    1  822    1    6  309    1 4160   11    1 4186    1\n",
      " 3990  184 2546  249    1  189  218  822 4208    4  170    1   69    1\n",
      "  217    1    1  202  237 4010  169 2715   14    1    4 1858    1    8\n",
      "    1  340  823  822 4083 3992   26   14 1799    1 6328   14   10    1\n",
      "    1 5062 9014   44 4112   10   43    1    8  718   37 9014   44  386\n",
      " 4112  385    1   14   10   43    1   40   37 1432   13    6  272   79\n",
      "    1   64], shape=(100,), dtype=int64)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n",
      "------\n",
      "tf.Tensor(\n",
      "[4102   56   17  132  125  848 9199   91 9483   83    1    1    1   14\n",
      "    1    1    1   64   10  977   91 1002 5464  600  125   10    1 9259\n",
      "  151  169    1    6  746   40  247 9742    1   83    1    4    1    6\n",
      "    1    4  277  125    1    1 1411  416   16    1    4 1216  251  169\n",
      "    1    1    1  169 4116   64    1    1   91 1797    1 1200  453   10\n",
      " 1467 4756 4942    1    1   83   10 1252 2721   18    4  112    1 2517\n",
      " 1146  837    1    4    1 9016   10    1 9911    1    1   11 5882  836\n",
      "    1 2689], shape=(100,), dtype=int64)\n",
      "tf.Tensor([1 0], shape=(2,), dtype=int64)\n",
      "------\n",
      "tf.Tensor(\n",
      "[   1  148   26  615   10  722 9128    6 1463    1   37 1144   10  855\n",
      "   11 1886    1   91   37   24 1426    6  254  335    1 9937    1  169\n",
      "  296  339    1   11    1   11  230 3989   23    1    8  148    1    1\n",
      " 7152    1   11 1156    1  129    8 9110  840    1 9188    1   11 9312\n",
      "    1 4054    1 1152   37  135    7    4    1   69  455    1    4  135\n",
      "    4 4027 1413    1   11    1   67    6 9258    1 1797    4  357   11\n",
      "  171  451 1148  108 1800    1 1412 8978  249    6  451 9828    1    1\n",
      "    1   91], shape=(100,), dtype=int64)\n",
      "tf.Tensor([0 1], shape=(2,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for a, b in ds_whole.take(5):\n",
    "    print(\"------\")\n",
    "    print(a)\n",
    "    print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25000"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list(ds_whole))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Na ukážke dát môžete vidieť, že každá recenzia je rôznej dĺžky slov. Keďže je nutné, aby vstupné polia mali rovnakú dĺžku, musíme ich doplniť nulami.\n",
    "\n",
    "Druhý rozmer je vždy dĺžky 2 pričom prvá hodnota znázorňuje percento pozitívneho ohodnotenia, druhá percento nehatívneho ohodnotenia recenzie."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "prekryv prvkov z train a test.\n",
    "Ci sa Neopakuju prvky.\n",
    "kolko trenovacich testovacich dat mame.\n",
    "explorativan analyza.\n",
    "min, mod, median -> dlzky reviews v pos a neg.\n",
    "\n",
    "add no. of start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ds_whole = ds_whole.apply(tf.data.experimental.unique())  # ==> { 1, 37, 2 }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'dropout_W')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-8d5fe4f4aad5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m model = tf.keras.Sequential([\n\u001b[1;32m     15\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEmbedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mVOCAB_SIZE\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m128\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_zero\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLSTM\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m48\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'sigmoid'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdropout_W\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'softmax'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m ])\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, time_major, unroll, **kwargs)\u001b[0m\n\u001b[1;32m    879\u001b[0m         \u001b[0mtime_major\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtime_major\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    880\u001b[0m         \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 881\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m    882\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    883\u001b[0m     self.state_spec = [\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1005\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_dropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eager_recurrent_dropout_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1007\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDropoutRNNCellMixin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1008\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1009\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_dropout_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, unit_forget_bias, kernel_regularizer, recurrent_regularizer, bias_regularizer, activity_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, implementation, return_sequences, return_state, go_backwards, stateful, unroll, **kwargs)\u001b[0m\n\u001b[1;32m   2539\u001b[0m         \u001b[0mstateful\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstateful\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2540\u001b[0m         \u001b[0munroll\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0munroll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2541\u001b[0;31m         **kwargs)\n\u001b[0m\u001b[1;32m   2542\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivity_regularizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactivity_regularizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2543\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_spec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/layers/recurrent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cell, return_sequences, return_state, go_backwards, stateful, unroll, time_major, **kwargs)\u001b[0m\n\u001b[1;32m    393\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'input_shape'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 395\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRNN\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    396\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcell\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    397\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturn_sequences\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_sequences\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/engine/base_layer.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    291\u001b[0m     }\n\u001b[1;32m    292\u001b[0m     \u001b[0;31m# Validate optional keyword arguments.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 293\u001b[0;31m     \u001b[0mgeneric_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidate_kwargs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    294\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    295\u001b[0m     \u001b[0;31m# Mutable properties\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/utils/generic_utils.py\u001b[0m in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    597\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mkwarg\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mallowed_kwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_message\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwarg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'dropout_W')"
     ]
    }
   ],
   "source": [
    "# ds = ds_whole\n",
    "\n",
    "ds = ds_test\n",
    "ds = ds.shuffle(buffer_size=10_000)\n",
    "# Bucketing, how the fuck do I sort padded batches ?\n",
    "ds = ds.apply(tf.data.experimental.bucket_by_sequence_length(\n",
    "    lambda el, _: tf.size(el),\n",
    "    [50, 60, 70, 80, 90, 100, 200, 300, 400, 500, 600, 700, 800, 900],\n",
    "    [32] * 15,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True\n",
    "))\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.LSTM(48, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds, epochs=1)\n",
    "\n",
    "while True:\n",
    "\tprint('Enter something:')\n",
    "\tinp = input()\n",
    "\tprint(model.predict(tokenizer.texts_to_sequences([inp.split(' ')])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
