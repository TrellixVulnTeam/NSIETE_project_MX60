{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "import random\n",
    "import tarfile\n",
    "import urllib.request\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded.\n"
     ]
    }
   ],
   "source": [
    "# Download dataset\n",
    "DATASET_URL = 'http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz'\n",
    "DATASET_DIR = 'dataset'\n",
    "DATASET_FILE_PATH = f'{DATASET_DIR}/aclImdb_v1.tar.gz'\n",
    "\n",
    "if not os.path.isfile(DATASET_FILE_PATH):\n",
    "    print(f'Downloading dataset into {DATASET_FILE_PATH} ...')\n",
    "    with urllib.request.urlopen(DATASET_URL) as response, open(DATASET_FILE_PATH, 'wb') as out_file:\n",
    "        shutil.copyfileobj(response, out_file)\n",
    "else:\n",
    "    print('Dataset already downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already extracted.\n"
     ]
    }
   ],
   "source": [
    "# Untar the dataset archive\n",
    "if not os.path.isdir(f'{DATASET_DIR}/aclImdb'):\n",
    "    with tarfile.open(DATASET_FILE_PATH) as archive:\n",
    "        print(f'Extracting \"{DATASET_FILE_PATH}\" to \"{DATASET_DIR}\" ...')\n",
    "        archive.extractall(DATASET_DIR)\n",
    "        print('Extraction finished.')\n",
    "else:\n",
    "    print('Dataset already extracted.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arguments\n",
    "VOCAB_SIZE = 1_000\n",
    "MAX_SENTENCE_LEN = 30\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data from folders\n",
    "TEST_FOLDER = f'{DATASET_DIR}/aclImdb/test'\n",
    "TEST_POSITIVE_FOLDER = f'{TEST_FOLDER}/pos'\n",
    "TEST_NEGATIVE_FOLDER = f'{TEST_FOLDER}/neg'\n",
    "\n",
    "TRAIN_FOLDER = f'{DATASET_DIR}/aclImdb/train'\n",
    "TRAIN_POSITIVE_FOLDER = f'{TRAIN_FOLDER}/pos'\n",
    "TRAIN_NEGATIVE_FOLDER = f'{TRAIN_FOLDER}/neg'\n",
    "TRAIN_UNSUPERVISED_FOLDER = f'{TRAIN_FOLDER}/unsup'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer(vocab_file, vocab_size, separator='\\n'):\n",
    "    vocab = open(vocab_file).read().split(separator)\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(vocab_size, oov_token=vocab_size)\n",
    "    tokenizer.fit_on_texts(vocab)\n",
    "    return tokenizer\n",
    "\n",
    "tokenizer = get_tokenizer(f'{DATASET_DIR}/aclImdb/imdb.vocab', VOCAB_SIZE+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_shifted_dataset_from_files(folders, shuffle=True):\n",
    "    files = map(lambda folder: glob.glob(f'{folder}/*'), folders)\n",
    "\n",
    "    labeled_files = map(lambda files_per_folder:\n",
    "                        map(lambda file_path:\n",
    "                            [open(file_path).read().split(' ')[:-1], open(file_path).read().split(' ')[1:]]\n",
    "                        , files_per_folder)\n",
    "                    , files)\n",
    "\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          *tokenizer.texts_to_sequences([example[1]])],\n",
    "                         flat_labeled_files)\n",
    "    return list(labeled_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_labeled_dataset_from_files(folders, label_map={'pos':[1, 0], 'neg': [0, 1]}, shuffle=True):\n",
    "    files = map(lambda folder: [glob.glob(f'{folder}/*'), f'{folder}'], folders)\n",
    "\n",
    "    # Assign label to every files based on folder they are in\n",
    "    labeled_files = map(lambda files_with_label:\n",
    "                        map(lambda file_path:\n",
    "                            [file_path, files_with_label[1].split('/')[-1]] # Take only the last folde from the folder path\n",
    "                        , files_with_label[0])\n",
    "                    , files)\n",
    "\n",
    "    # flatten list\n",
    "    flat_labeled_files = []\n",
    "    for lf in labeled_files:\n",
    "        for fl in lf:\n",
    "            flat_labeled_files.append(fl)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(flat_labeled_files)\n",
    "\n",
    "    # read file contents\n",
    "    labeled_texts = map(lambda example: [open(example[0]).read().split(' ')[:MAX_SENTENCE_LEN], example[1]], flat_labeled_files)\n",
    "\n",
    "    # tokenize texts\n",
    "    labeled_tokens = map(lambda example: [*tokenizer.texts_to_sequences([example[0]]),\n",
    "                                          label_map[example[1]]], labeled_texts)\n",
    "    return list(labeled_tokens), len(flat_labeled_files)\n",
    "\n",
    "cls_test_ds, num_test_samples = create_labeled_dataset_from_files([f'{TEST_POSITIVE_FOLDER}', f'{TEST_NEGATIVE_FOLDER}, {TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "cls_train_ds, num_train_samples = create_labeled_dataset_from_files([f'{TRAIN_POSITIVE_FOLDER}', f'{TRAIN_NEGATIVE_FOLDER}'])\n",
    "\n",
    "def cls_test_gen():\n",
    "    for el in cls_test_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "def cls_train_gen():\n",
    "    for el in cls_train_ds:\n",
    "        yield (el[0], el[1])\n",
    "\n",
    "ds_test = tf.data.Dataset.from_generator(lambda: cls_test_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = tf.data.Dataset.from_generator(lambda: cls_train_gen(),\n",
    "                                        (tf.int64, tf.int64)).repeat()\n",
    "ds_train = ds_train.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)\n",
    "\n",
    "ds_test = ds_test.padded_batch(\n",
    "    BATCH_SIZE,\n",
    "    padded_shapes=([None], [2]),\n",
    "    drop_remainder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 781 steps, validate for 781 steps\n",
      "Epoch 1/5\n",
      "781/781 [==============================] - 57s 73ms/step - loss: 0.6699 - accuracy: 0.5727 - val_loss: 0.6347 - val_accuracy: 0.6367\n",
      "Epoch 2/5\n",
      "781/781 [==============================] - 52s 66ms/step - loss: 0.6030 - accuracy: 0.6669 - val_loss: 0.6088 - val_accuracy: 0.6637\n",
      "Epoch 3/5\n",
      "781/781 [==============================] - 53s 67ms/step - loss: 0.5815 - accuracy: 0.6869 - val_loss: 0.6135 - val_accuracy: 0.6630\n",
      "Epoch 4/5\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.5700 - accuracy: 0.6965 - val_loss: 0.6120 - val_accuracy: 0.6654\n",
      "Epoch 5/5\n",
      "781/781 [==============================] - 54s 69ms/step - loss: 0.5619 - accuracy: 0.7032 - val_loss: 0.6117 - val_accuracy: 0.6665\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f5c0b7fa710>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Embedding(input_dim=VOCAB_SIZE+2, output_dim=128, mask_zero=True),\n",
    "    tf.keras.layers.Bidirectional(\n",
    "        tf.keras.layers.LSTM(48, activation='sigmoid')),\n",
    "    tf.keras.layers.Dense(2, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(lr=1e-3),\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(ds_train,\n",
    "        epochs=5,\n",
    "        shuffle=True,\n",
    "        validation_data=ds_test,\n",
    "        steps_per_epoch=num_train_samples // BATCH_SIZE,\n",
    "        validation_steps=num_test_samples // BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "\tprint('Enter something:')\n",
    "\tinp = input()\n",
    "\tprint(model.predict(tokenizer.texts_to_sequences([inp.split(' ')])))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
